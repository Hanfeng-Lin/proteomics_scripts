{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import ttest_rel\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "FILE=\"homo_sapiens_reviewed_NGT20-12\"\n",
    "imputation_option=True\n",
    "\n",
    "pg_matrix = FILE + \".pg_matrix.tsv\"\n",
    "pr_matrix = FILE + \".pr_matrix.tsv\"\n",
    "\n",
    "df = pd.read_csv(pg_matrix,sep=\"\\t\",index_col=0)\n",
    "df_peptide = pd.read_csv(pr_matrix, sep=\"\\t\", index_col=0)\n",
    "print(\"protein before decontamination: \"+str(df.shape))\n",
    "#remove proteins from contaminant database\n",
    "#contaminants=[\"ALBU_HUMAN\", \"AMY1A_HUMAN\", \"AMY1B_HUMAN\", \"AMY1C_HUMAN\", \"ANT3_HUMAN\", \"ANXA5_HUMAN\", \"B2MG_HUMAN\", \"BID_HUMAN\", \"CAH1_HUMAN\", \"CAH2_HUMAN\", \"CATA_HUMAN\", \"CATD_HUMAN\", \"CATG_HUMAN\", \"CO5_HUMAN\", \"CRP_HUMAN\", \"CYB5_HUMAN\", \"CYC_HUMAN\", \"EGF_HUMAN\", \"FABPH_HUMAN\", \"GELS_HUMAN\", \"GSTA1_HUMAN\", \"GSTP1_HUMAN\", \"HBA_HUMAN\", \"HBB_HUMAN\", \"HBEGF_HUMAN\", \"IGF2_HUMAN\", \"IL8_HUMAN\", \"INHBA_HUMAN\", \"INHBB_HUMAN\", \"KCRM_HUMAN\", \"LALBA_HUMAN\", \"LEP_HUMAN\", \"LYSC_HUMAN\", \"MYG_HUMAN\", \"NEDD8_HUMAN\", \"NQO1_HUMAN\", \"NQO2_HUMAN\", \"PDGFB_HUMAN\", \"PPIA_HUMAN\", \"PRDX1_HUMAN\", \"RASH_HUMAN\", \"RET4_HUMAN\", \"RS27A_HUMAN\", \"SODC_HUMAN\", \"SUMO1_HUMAN\", \"HARS1_HUMAN\", \"TAU_HUMAN\", \"THIO_HUMAN\", \"TNFA_HUMAN\", \"TRFE_HUMAN\", \"TRFL_HUMAN\", \"UB2E1_HUMAN\", \"UBE2C_HUMAN\", \"K1C26_HUMAN\", \"K2C3_HUMAN\", \"K1C15_HUMAN\", \"K2C79_HUMAN\", \"K2C6B_HUMAN\", \"K1C17_HUMAN\", \"K2C4_HUMAN\", \"K2C73_HUMAN\", \"K2C71_HUMAN\", \"K2C7_HUMAN\", \"K2C8_HUMAN\", \"K1C39_HUMAN\", \"K1C18_HUMAN\", \"K1C28_HUMAN\", \"K1C16_HUMAN\", \"K2C1_HUMAN\", \"K2C5_HUMAN\", \"K2C80_HUMAN\", \"K2C1B_HUMAN\", \"K2C75_HUMAN\", \"K2C6A_HUMAN\", \"K2C72_HUMAN\", \"K1C24_HUMAN\", \"K1C19_HUMAN\", \"K2C74_HUMAN\", \"K1C27_HUMAN\", \"K1C20_HUMAN\", \"K1C9_HUMAN\", \"K1C23_HUMAN\", \"K1C12_HUMAN\", \"K1C14_HUMAN\", \"K2C6C_HUMAN\", \"K1C10_HUMAN\", \"K1C13_HUMAN\", \"K22O_HUMAN\", \"K1C25_HUMAN\", \"K2C78_HUMAN\", \"K22E_HUMAN\", \"K1C40_HUMAN\", \"KRT85_HUMAN\", \"KRT38_HUMAN\", \"KRT34_HUMAN\", \"KRT86_HUMAN\", \"KRT35_HUMAN\", \"KT33B_HUMAN\", \"KRT81_HUMAN\", \"KRT37_HUMAN\", \"KT33A_HUMAN\", \"KRT83_HUMAN\", \"K1H1_HUMAN\", \"KRT82_HUMAN\", \"K1H2_HUMAN\", \"KRT36_HUMAN\", \"KRT84_HUMAN\"]\n",
    "contaminants=[\"P02768\", \"P0DUB6\", \"P0DTE7\", \"P0DTE8\", \"P01008\", \"P08758\", \"P61769\", \"P55957\", \"P00915\", \"P00918\", \"P04040\", \"P07339\", \"P08311\", \"P01031\", \"P02741\", \"P00167\", \"P99999\", \"P01133\", \"P05413\", \"P06396\", \"P08263\", \"P09211\", \"P69905\", \"P68871\", \"Q99075\", \"P01344\", \"P10145\", \"P08476\", \"P09529\", \"P06732\", \"P00709\", \"P41159\", \"P61626\", \"P02144\", \"Q15843\", \"P15559\", \"P16083\", \"P01127\", \"P62937\", \"Q06830\", \"P01112\", \"P02753\", \"P62979\", \"P00441\", \"P63165\", \"P12081\", \"P10636\", \"P10599\", \"P01375\", \"P02787\", \"P02788\", \"P51965\", \"O00762\", \"Q7Z3Y9\", \"P12035\", \"P19012\", \"Q5XKE5\", \"P04259\", \"Q04695\", \"P19013\", \"Q86Y46\", \"Q3SY84\", \"P08729\", \"P05787\", \"Q6A163\", \"P05783\", \"Q7Z3Y7\", \"P08779\", \"P04264\", \"P13647\", \"Q6KB66\", \"Q7Z794\", \"O95678\", \"P02538\", \"Q14CN4\", \"Q2M2I5\", \"P08727\", \"Q7RTS7\", \"Q7Z3Y8\", \"P35900\", \"P35527\", \"Q9C075\", \"Q99456\", \"P02533\", \"P48668\", \"P13645\", \"P13646\", \"Q01546\", \"Q7Z3Z0\", \"Q8N1N4\", \"P35908\", \"Q6A162\", \"P78386\", \"O76015\", \"O76011\", \"O43790\", \"Q92764\", \"Q14525\", \"Q14533\", \"O76014\", \"O76009\", \"P78385\", \"Q15323\", \"Q9NSB4\", \"Q14532\", \"O76013\", \"Q9NSB2\"]\n",
    "df = df[~df.index.isin(contaminants)]\n",
    "print(\"protein after decontamination: \"+str(df.shape))\n",
    "\n",
    "print(\"peptide before decontamination:\"+str(df_peptide.shape))\n",
    "df_peptide = df_peptide[~df_peptide.index.isin(contaminants)]\n",
    "print(\"peptide after decontamination:\"+str(df_peptide.shape))\n",
    "\n",
    "df_peptide.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_columns = {\n",
    "    'DMSO': [x for x in df.columns if \"DMSO\" in x],\n",
    "    'NGT20-12_1uM': [x for x in df.columns if \"NGT20-12_1uM\" in x],\n",
    "    }\n",
    "\n",
    "print(group_columns)\n",
    "for key in group_columns:\n",
    "    print(str(key)+\": \"+str(len(group_columns[key])))\n",
    "    for name in group_columns[key]:\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation\n",
    "import logging\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)  # Set logging level to INFO\n",
    "for handler in logger.handlers[:]:\n",
    "    logger.removeHandler(handler)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "\n",
    "def imputation(df, treated_group_name, control_group_name,peptide_count_cutoff=3):\n",
    "    control_group = group_columns[control_group_name]\n",
    "    treated_group = group_columns[treated_group_name]\n",
    "\n",
    "    low_1_percentile = np.nanpercentile(df[treated_group].values.flatten(), 1)\n",
    "    logger.info(f\"low_1_percentile in treated group is {low_1_percentile}\")\n",
    "\n",
    "    treated_row_cv = (df.loc[:, treated_group].dropna().std(axis=1) / df.loc[:, treated_group].dropna().mean(axis=1)).mean()\n",
    "    logger.info(f\"mean CV in treated group is {treated_row_cv}\")\n",
    "\n",
    "    peptide_counts = df_peptide.index.value_counts()\n",
    "\n",
    "    imputation_list = []\n",
    "\n",
    "    for protein in df.index:\n",
    "        # Control group handling\n",
    "        control_values = df.loc[protein, control_group]\n",
    "        if control_values.isna().sum() >= len(control_group) / 2:\n",
    "            # Discard this protein\n",
    "            df.drop(index=protein, inplace=True)\n",
    "            logger.info(f\"Protein {protein} discarded due to missing values in control group.\")\n",
    "            continue\n",
    "        else:\n",
    "            # Impute control group based on its own mean and std\n",
    "            control_mean = control_values.mean(skipna=True)\n",
    "            control_std = control_values.std(skipna=True)\n",
    "            missing_indices = control_values.index[control_values.isna()]\n",
    "            df.loc[protein, missing_indices] = truncnorm.rvs(\n",
    "                (0 - control_mean) / control_std,  # Lower bound normalized\n",
    "                np.inf,                           # Upper bound normalized\n",
    "                loc=control_mean, scale=control_std, size=len(missing_indices))\n",
    "            logger.debug(f\"Imputed missing values in control group for protein {protein}.\")\n",
    "\n",
    "        # Treated group handling\n",
    "        treated_values = df.loc[protein, treated_group]\n",
    "        if treated_values.isna().sum() < len(treated_group) / 2:\n",
    "            # Impute treated group based on its own mean and std\n",
    "            treated_mean = treated_values.mean(skipna=True)\n",
    "            treated_std = treated_values.std(skipna=True)\n",
    "            missing_indices = treated_values.index[treated_values.isna()]\n",
    "            df.loc[protein, missing_indices] = truncnorm.rvs(\n",
    "                (0 - treated_mean) / treated_std,  # Lower bound normalized\n",
    "                np.inf,                           # Upper bound normalized\n",
    "                loc=treated_mean, scale=treated_std, size=len(missing_indices))\n",
    "            logger.debug(f\"Imputed missing values in treated group for protein {protein}.\")\n",
    "        elif treated_values.isna().sum() >= len(treated_group) / 2:\n",
    "            if treated_values.isna().sum() == len(treated_group):\n",
    "                peptide_count = peptide_counts.get(protein, 0)\n",
    "                if peptide_count > peptide_count_cutoff:\n",
    "                    if not df_peptide.loc[protein, treated_group].empty:\n",
    "                        # Perform peptide-level t-test\n",
    "                        fold_changes = [df_peptide.loc[protein, treated_group].iloc[row].mean(skipna=True) / \n",
    "                                        df_peptide.loc[protein, control_group].iloc[row].mean(skipna=True)\n",
    "                                        for row in range(len(df_peptide.loc[protein]))]\n",
    "                        p_values = [\n",
    "                            ttest_ind(\n",
    "                                df_peptide.loc[protein, treated_group].iloc[row].dropna(),\n",
    "                                df_peptide.loc[protein, control_group].iloc[row].dropna(),\n",
    "                                equal_var=True\n",
    "                            ).pvalue\n",
    "                            for row in range(len(df_peptide.loc[protein]))\n",
    "                        ]\n",
    "                        logger.info(f\"Protein {protein} peptide fold changes : {fold_changes}\")\n",
    "                        logger.info(f\"Protein {protein} peptide p-values : {p_values}\")\n",
    "                        \n",
    "                        if np.nanmedian(p_values) < 0.05 or (np.isnan(p_values).all() and not np.isnan(fold_changes).all()):\n",
    "                            df.loc[protein, treated_group] = np.random.uniform(\n",
    "                                low=low_1_percentile * 0.5,\n",
    "                                high=low_1_percentile * 1.5,\n",
    "                                size=len(treated_group)\n",
    "                            )\n",
    "                            imputation_list.append(protein)\n",
    "                            logger.info(f\"Protein {protein} has {str(peptide_count)} peptides, missing values imputed in treated group after significant or NaN p-values.\")\n",
    "                        else:\n",
    "                            logger.info(f\"Protein {protein} has {str(peptide_count)} peptides, but no imputation performed since peptide p-values are not significant, or there is no calculatable fold changes.\")\n",
    "                    else:\n",
    "                        df.loc[protein, treated_group] = np.random.uniform(\n",
    "                            low=low_1_percentile * 0.5, \n",
    "                            high=low_1_percentile * 1.5, \n",
    "                            size=len(treated_group)\n",
    "                        )\n",
    "                        imputation_list.append(protein)\n",
    "                        logger.info(f\"Protein {protein} has {str(peptide_count)} peptides, missing values imputed in treated group (no peptide data).\")\n",
    "                else:\n",
    "                    logger.info(f\"Protein {protein} only has {str(peptide_count)} peptides, skipped imputation from minimum.\")\n",
    "            else:\n",
    "                peptide_count = peptide_counts.get(protein, 0)\n",
    "                if peptide_count > peptide_count_cutoff:\n",
    "                    treated_mean = df.loc[protein, treated_group].mean()\n",
    "                    missing_indices = treated_values.index[treated_values.isna()]\n",
    "                    df.loc[protein, missing_indices] = truncnorm.rvs(\n",
    "                        (0 - treated_mean) / treated_row_cv*treated_mean,  # Lower bound normalized\n",
    "                        np.inf,                           # Upper bound normalized\n",
    "                        loc=treated_mean, scale=treated_row_cv*treated_mean, size=len(missing_indices))\n",
    "                    imputation_list.append(protein)\n",
    "                    logger.info(f\"Protein {protein} has {str(peptide_count)} peptides, missing values imputed in treated group using CV-based imputation.\")\n",
    "                else:\n",
    "                    logger.info(f\"Protein {protein} only has {str(peptide_count)} peptides, skipped CV-based imputation.\")\n",
    "\n",
    "    logger.info(f\"Imputation completed for {treated_group_name} vs {control_group_name}. {len(imputation_list)} proteins with >50% missing value in the treated group were imputed .\")\n",
    "    return df, imputation_list\n",
    "\n",
    "\n",
    "# Calculate FC value and append to the right side\n",
    "def calculate_average_FC_value(df, treated_group_name, control_group_name):\n",
    "    control_group = group_columns[control_group_name]\n",
    "    treated_group = group_columns[treated_group_name]\n",
    "    \n",
    "    control_avg = df[control_group].mean(axis=1)\n",
    "    treated_avg = df[treated_group].mean(axis=1)\n",
    "    \n",
    "    FC_values = treated_avg / control_avg\n",
    "    \n",
    "    FC_column_name = f'FC_{treated_group_name}_vs_{control_group_name}'\n",
    "    df[FC_column_name] = FC_values\n",
    "    \n",
    "    log2FC_values = np.log2(FC_values)\n",
    "    \n",
    "    log2FC_column_name = f'log2FC_{treated_group_name}_vs_{control_group_name}'\n",
    "    df[log2FC_column_name] = log2FC_values\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Setting reference group here. Reference group should have the least missing value, a.k.a. more complex proteome. For TPD, this should be DMSO, while for AP-MS, this should be treatment group\n",
    "reference_group = \"DMSO\"\n",
    "\n",
    "imputation_dict={}\n",
    "\n",
    "\n",
    "\n",
    "comparison_matrix = [] # empty list means all other groups are compared with one reference group\n",
    "# Uncomment the following code if you want to customize comparison groups:\n",
    "\"\"\"\n",
    "comparison_matrix = [\n",
    "    ['NGT20-12_1uM', 'DMSO'],\n",
    "] # treatment_group : reference_group\n",
    "\"\"\"\n",
    "if not comparison_matrix:\n",
    "    for key in group_columns:\n",
    "        if key != reference_group:  # Skip the reference group\n",
    "            if imputation_option:\n",
    "                df, imputation_list = imputation(df, key, reference_group)  # Only proteins with >half missing values in treated group are in the imputation list\n",
    "                imputation_dict[key+\"_vs_\"+reference_group] = imputation_list\n",
    "            df = calculate_average_FC_value(df, key, reference_group)\n",
    "else:\n",
    "    for pair in comparison_matrix:\n",
    "        if imputation_option:\n",
    "            df, imputation_list = imputation(df, pair[0], pair[1])\n",
    "            imputation_dict[pair[0]+\"_vs_\"+pair[1]] = imputation_list\n",
    "        df = calculate_average_FC_value(df, pair[0], pair[1])\n",
    "        \n",
    "# Uncomment the following code if you want to customize comparison groups:\n",
    "\n",
    "print(imputation_dict)\n",
    "df.to_csv(\"imputation.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform a student t-test\n",
    "def student_t_test(row, treated_group_name, control_group_name,min_valid_value=3): # valid value = Not NaN\n",
    "    treated_values = pd.to_numeric(row[group_columns[treated_group_name]], errors='coerce').dropna().values\n",
    "    control_values = pd.to_numeric(row[group_columns[control_group_name]], errors='coerce').dropna().values\n",
    "    \n",
    "    if not imputation_option:\n",
    "        if len(treated_values) <min_valid_value or len(control_values) <min_valid_value:\n",
    "            return np.nan\n",
    "    \n",
    "    return ttest_ind(treated_values, control_values, equal_var=True).pvalue\n",
    "\n",
    "# Iterate over group_columns to calculate p-values\n",
    "if not comparison_matrix:\n",
    "    for key in group_columns:\n",
    "        if key != reference_group:  # Skip the reference group\n",
    "            pvalue_column_name = f'Pvalue_{key}_vs_{reference_group}'\n",
    "            df[pvalue_column_name] = df.apply(\n",
    "                student_t_test,\n",
    "                axis=1,\n",
    "                treated_group_name=key,\n",
    "                control_group_name=reference_group\n",
    "            )\n",
    "else:\n",
    "    for pair in comparison_matrix:\n",
    "        pvalue_column_name = f'Pvalue_{pair[0]}_vs_{pair[1]}'\n",
    "        df[pvalue_column_name] = df.apply(\n",
    "                student_t_test,\n",
    "                axis=1,\n",
    "                treated_group_name=pair[0],\n",
    "                control_group_name=pair[1])\n",
    "\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Benjamini-Hochberg FDR correction, handling NaN values\n",
    "def apply_bh_fdr(p_values):\n",
    "    mask = np.isfinite(p_values)\n",
    "    p_values_corrected = np.full(p_values.shape, np.nan)\n",
    "    p_values_corrected[mask] = multipletests(p_values[mask], method='fdr_bh')[1]\n",
    "    return p_values_corrected\n",
    "\n",
    "# Apply BH FDR correction for each set of p-values\n",
    "if not comparison_matrix:\n",
    "    for key in group_columns:\n",
    "        if key != reference_group:  # Skip the reference group\n",
    "            bhFDR_column_name = f'bh_FDR_{key}_vs_{reference_group}'\n",
    "            df[bhFDR_column_name] = apply_bh_fdr(df[f'Pvalue_{key}_vs_{reference_group}'])\n",
    "else:\n",
    "    for pair in comparison_matrix:\n",
    "        bhFDR_column_name = f'bh_FDR_{pair[0]}_vs_{pair[1]}'\n",
    "        df[bhFDR_column_name] = apply_bh_fdr(df[f'Pvalue_{pair[0]}_vs_{pair[1]}'])\n",
    "\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(FILE.split(\".\")[0]+\"_analyzed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volcano Plot\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from adjustText import adjust_text\n",
    "\n",
    "def volcano_plot(treatment_group, control_group, logFC_cutoff=1, logFC_cutoff2=None, FDR_cutoff=0.05, file_suffix=\"\", highlight_genes=[], protein_level_cutoff=None, xlim=[], imputation_option=imputation_option):\n",
    "    logFC=\"log2FC_\"+treatment_group+\"_vs_\"+control_group\n",
    "    FDR=\"bh_FDR_\"+treatment_group+\"_vs_\"+control_group\n",
    "    plt.figure(figsize=(12, 9))\n",
    "\n",
    "    plt.scatter(x=df[logFC],y=df[FDR].apply(lambda x:-np.log10(x)),s=1, color=\"grey\")\n",
    "\n",
    "    down = df[(df[logFC]<=-logFC_cutoff)&(df[FDR]<=FDR_cutoff)]\n",
    "    if logFC_cutoff2:    \n",
    "        slight_down = df[(df[logFC]>-logFC_cutoff) & (df[logFC]<=-logFC_cutoff2)&(df[FDR]<=FDR_cutoff)]\n",
    "    else:\n",
    "        slight_down = None\n",
    "\n",
    "    up = df[(df[logFC]>=logFC_cutoff)&(df[FDR]<=FDR_cutoff)]\n",
    "    \n",
    "    plt.scatter(x=up[logFC],y=up[FDR].apply(lambda x:-np.log10(x)),s=3,label=\"Up-regulated\",color=\"red\")\n",
    "\n",
    "    if not (logFC_cutoff2 or protein_level_cutoff):\n",
    "        plt.scatter(x=down[logFC],y=down[FDR].apply(lambda x:-np.log10(x)),s=3,label=\"Down-regulated\",color=\"blue\")\n",
    "    \n",
    "    if logFC_cutoff2:\n",
    "        plt.scatter(x=down[logFC],y=down[FDR].apply(lambda x:-np.log10(x)),s=3,label=\">50% Down-regulated\",color=\"blue\")\n",
    "        plt.scatter(x=slight_down[logFC],y=slight_down[FDR].apply(lambda x:-np.log10(x)),s=3,label=\"30-50% Down-regulated\",color=\"turquoise\")\n",
    "    \n",
    "    if protein_level_cutoff:\n",
    "        lowabundance_down=df[(df[logFC]<=-logFC_cutoff) & (df[FDR]<=FDR_cutoff) & (df[group_columns[control_group]].mean(axis=1)<1000)]\n",
    "        plt.scatter(x=down[logFC],y=down[FDR].apply(lambda x:-np.log10(x)),s=3,label=\"Down-regulated\",color=\"blue\")\n",
    "        plt.scatter(x=lowabundance_down[logFC],y=lowabundance_down[FDR].apply(lambda x:-np.log10(x)),s=3,label=\"Down-regulated, protein level<\"+str(protein_level_cutoff),color=\"turquoise\")\n",
    "\n",
    "    if highlight_genes:\n",
    "        highlight = df.loc[highlight_genes]\n",
    "        plt.scatter(x=highlight[logFC], y=highlight[FDR].apply(lambda x:-np.log10(x)), color='green')\n",
    "    else:\n",
    "        highlight = None\n",
    "\n",
    "    if imputation_option:\n",
    "        imputation_proteins = df.loc[imputation_dict[treatment_group+\"_vs_\"+reference_group]]\n",
    "        plt.scatter(x=imputation_proteins[logFC], y=imputation_proteins[FDR].apply(lambda x:-np.log10(x)), s=3, color='orange', label=\"Imputation from missing value\")\n",
    "    else:\n",
    "        imputation_proteins = None\n",
    "    \n",
    "    texts=[]\n",
    "    for i,r in pd.concat([up,down,slight_down,highlight]).drop_duplicates().iterrows():\n",
    "        #texts.append(plt.text(x=r[logFC],y=-np.log10(r[FDR]),s=i.split(\"_\")[0]))\n",
    "        texts.append(plt.text(x=r[logFC],y=-np.log10(r[FDR]),s=r['Genes'], size=14))\n",
    "\n",
    "    \n",
    "    plt.xlim(xlim[0],xlim[1])\n",
    "    plt.ylim(-0.5,5.5)\n",
    "\n",
    "    plt.ylabel(\"-logFDR\", size=16)\n",
    "    plt.title(logFC.split(\"_\", maxsplit=1)[1]+\"\\nn=\"+str(len(df[FDR].dropna())),size=24)\n",
    "    plt.axvline(-logFC_cutoff,color=\"grey\",linestyle=\"--\")\n",
    "    if logFC_cutoff2:\n",
    "        plt.axvline(-logFC_cutoff2,color=\"grey\",linestyle=\"--\") \n",
    "    plt.axvline(logFC_cutoff,color=\"grey\",linestyle=\"--\")\n",
    "    plt.axhline(-np.log10(FDR_cutoff),color=\"grey\",linestyle=\"--\")\n",
    "    plt.legend(loc=\"upper right\", fontsize=16)\n",
    "    logFC=logFC[:3]+\"\\u2082\"+logFC[4:]\n",
    "    plt.xlabel(logFC, labelpad=10, size=16)\n",
    "\n",
    "\n",
    "    adjust_text(texts, force_text=(0.5,1),force_static =(1,2),arrowprops=dict(arrowstyle=\"-\", color='black', lw=0.5))\n",
    "\n",
    "    imputation_suffix=\"_no_imputation\"\n",
    "    if imputation_option:\n",
    "        imputation_suffix=\"_imputation\"\n",
    "    rep_suffix=\"_\"+str(len(group_columns[reference_group]))+\"rep\"\n",
    "    plt.savefig(logFC+file_suffix+imputation_suffix+rep_suffix+'.png', transparent=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# volcano plot time!!\n",
    "for key in group_columns:\n",
    "    if key != reference_group:  # Skip the reference group\n",
    "        volcano_plot(key, reference_group, FDR_cutoff=0.01, highlight_genes=[\"P15498\"], xlim=[-10,10], file_suffix=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FILE.split(\".\")[0]+\"_analyzed.csv\",sep=\",\",index_col=0)\n",
    "\n",
    "# Bubble plot table for x=treatment, y=protein, color=FC, size=FDR\n",
    "df.head()\n",
    "df = df.drop(columns=df.filter(like=\"10uM\").columns)\n",
    "\n",
    "\n",
    "# Identify log2FC and bh_FDR columns\n",
    "log2FC_columns = df.filter(regex=\"^log2FC_\").columns\n",
    "bh_FDR_columns = df.filter(regex=\"^bh_FDR_\").columns\n",
    "\n",
    "# Extract suffixes for pairing\n",
    "log2FC_suffixes = [col.split(\"_\", 1)[1] for col in log2FC_columns]\n",
    "bh_FDR_suffixes = [col.split(\"_\", 2)[2] for col in bh_FDR_columns]\n",
    "\n",
    "print(log2FC_suffixes)\n",
    "print(bh_FDR_suffixes)\n",
    "\n",
    "# Create a mask for filtering rows\n",
    "mask = pd.DataFrame(\n",
    "    {\n",
    "        suffix: (df[f\"log2FC_{suffix}\"] < -1) & (df[f\"bh_FDR_{suffix}\"] < 0.01)\n",
    "        for suffix in log2FC_suffixes\n",
    "    }\n",
    ").any(axis=1)\n",
    "\n",
    "# Apply the mask to filter rows\n",
    "df_protein_downreg = df[mask]\n",
    "df_protein_downreg.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.cm import get_cmap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans  # Use KMeans for clustering\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram, set_link_color_palette\n",
    "from matplotlib.colors import rgb2hex\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "def dendro(df_protein_downreg, SAR, fig_height=10, fig_width=50):\n",
    "\n",
    "    # Melt the DataFrame for plotting\n",
    "    melted_log2FC = pd.melt(\n",
    "        df_protein_downreg,\n",
    "        id_vars=[\"First.Protein.Description\"],\n",
    "        value_vars=[col for col in df_protein_downreg.columns if col.startswith(\"log2FC\")],\n",
    "        var_name=\"log2FC_Column\",\n",
    "        value_name=\"log2FC\",\n",
    "    )\n",
    "    melted_log2FC[\"Suffix\"] = melted_log2FC[\"log2FC_Column\"].str.split(\"_\", n=1).str[1]\n",
    "    melted_log2FC[\"Group\"] = melted_log2FC[\"Suffix\"].map(suffix_to_group)\n",
    "    \n",
    "    # Map bh_FDR values and calculate -log10 scale\n",
    "    melted_log2FC[\"bh_FDR\"] = melted_log2FC.apply(\n",
    "        lambda row: df_protein_downreg[f\"bh_FDR_{row['Suffix']}\"].iloc[row.name % len(df_protein_downreg)], axis=1\n",
    "    )\n",
    "    melted_log2FC[\"bh_FDR_log10\"] = -np.log10(melted_log2FC[\"bh_FDR\"])\n",
    "\n",
    "\n",
    "    # Pivot to get a matrix of First.Protein.Description vs. log2FC values for clustering\n",
    "    pivot_df = melted_log2FC.pivot_table(\n",
    "        index=\"First.Protein.Description\",\n",
    "        columns=\"Suffix\",\n",
    "        values=\"log2FC\"\n",
    "    )\n",
    "    pivot_df = pivot_df.fillna(1)\n",
    "\n",
    "    # Compute pairwise distances and hierarchical clustering\n",
    "    distance_threshold = 3.5 # Set your cutoff distance here\n",
    "    linkage_matrix = linkage(pivot_df, method=\"ward\")  # You can use \"average\", \"complete\", etc.\n",
    "\n",
    "    # Define clusters using a distance threshold\n",
    "    cluster_labels = fcluster(linkage_matrix, t=distance_threshold, criterion=\"distance\")\n",
    "\n",
    "    # Assign cluster labels to the pivot table\n",
    "    pivot_df[\"Cluster\"] = cluster_labels\n",
    "\n",
    "    # Map cluster labels back to the original DataFrame\n",
    "    cluster_mapping = pivot_df[\"Cluster\"].to_dict()\n",
    "    melted_log2FC[\"Cluster\"] = melted_log2FC[\"First.Protein.Description\"].map(cluster_mapping)\n",
    "\n",
    "    # Sort data by cluster (to reorder x-axis based on k-means clusters)\n",
    "    melted_log2FC[\"Cluster\"] = melted_log2FC[\"Cluster\"].astype(str)  # Convert to string for categorical color coding\n",
    "    melted_log2FC[\"Suffix\"] = melted_log2FC[\"Suffix\"].astype(str)\n",
    "\n",
    "\n",
    "    melted_log2FC.to_csv(\"temp.csv\")\n",
    "\n",
    "    fig = plt.figure(figsize=(fig_width,fig_height))\n",
    "\n",
    "    dendro = dendrogram(\n",
    "        linkage_matrix,\n",
    "        labels=pivot_df.index,\n",
    "        leaf_rotation=90,\n",
    "        distance_sort=\"ascending\",\n",
    "        color_threshold=0,\n",
    "        above_threshold_color=\"black\",\n",
    "\n",
    "        )\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return melted_log2FC, pivot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example SAR dictionary and DataFrame setup (unchanged)\n",
    "SAR = {\n",
    "    \"benzene\": [\"NGT20-11\", \"NGT20-12\", \"NGT20-19\", \"NGT20-110\", \"NGT20-111\"],\n",
    "    \"thiophene\": [\"NGT20-13\", \"NGT20-14\", \"NGT20-117\", \"NGT20-118\"],\n",
    "    \"pyridine\": [\"NGT20-15\", \"NGT20-16\", \"NGT20-113\", \"NGT20-114\"],\n",
    "    \"pyrimidine\": [\"NGT20-17\", \"NGT20-18\"],\n",
    "    \"indole\": [\"NGT20-119\", \"NGT20-120\"],\n",
    "}\n",
    "\n",
    "melted_log2FC, pivot_df = dendro(df_protein_downreg,SAR)\n",
    "print(melted_log2FC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import numpy as np\n",
    "from matplotlib.gridspec import GridSpec, GridSpecFromSubplotSpec\n",
    "\n",
    "def bubble_dendro_plot(melted_log2FC, pivot_df, SAR=SAR, SAR_suffix=\"\", fig_width=50, fig_height=25, dendro_bubble_height_ratio=[1,3], bubble_legend_width_ratio=[20,1]):\n",
    "\n",
    "    for key in SAR:\n",
    "        SAR[key] = list(map(lambda s: s + SAR_suffix, SAR[key]))\n",
    "\n",
    "    suffix_to_group = {suffix: cluster for cluster, suffixes in SAR.items() for suffix in suffixes}\n",
    "    # Create a linkage matrix for hierarchical clustering\n",
    "    linkage_matrix = linkage(pivot_df, method=\"ward\")\n",
    "    \n",
    "    # Create the figure and axis for the subplots\n",
    "    fig = plt.figure(figsize=(fig_width,fig_height))\n",
    "    gs = GridSpec(2, 2, figure=fig, height_ratios=dendro_bubble_height_ratio, width_ratios=bubble_legend_width_ratio)\n",
    "    \n",
    "    # First subplot for the dendrogram (use position 211 for the first plot)\n",
    "    ax_dendro = fig.add_subplot(gs[0,0])\n",
    "    #ax_dendro.axis(\"off\")\n",
    "    dendro = dendrogram(\n",
    "        linkage_matrix,\n",
    "        labels=pivot_df.index,\n",
    "        leaf_rotation=90,\n",
    "        distance_sort=\"ascending\",\n",
    "        color_threshold=0,\n",
    "        above_threshold_color=\"black\",\n",
    "        ax=ax_dendro\n",
    "        )\n",
    "    # Extract the order of labels (leaf order) for custom coloring\n",
    "    dendrogram_order = dendro[\"ivl\"]\n",
    "    \n",
    "    # Add title and labels\n",
    "    ax_dendro.set_title(\"\", fontsize=16)\n",
    "    ax_dendro.set_xticks([])\n",
    "    ax_dendro.set_ylabel(\"Distance\", fontsize=20)\n",
    "    \n",
    "    # Define group_colors for coloring y-axis labels based on SAR\n",
    "    unique_groups = melted_log2FC[\"Group\"].dropna().unique()  # Groups based on SAR\n",
    "    group_colors = {group: get_cmap(\"tab10\")(i / len(unique_groups)) for i, group in enumerate(unique_groups)}\n",
    "    melted_log2FC[\"Group_Color\"] = melted_log2FC[\"Group\"].map(group_colors)\n",
    "    \n",
    "    # Re-order y-axis by suffix (based on SAR dictionary)\n",
    "    ordered_suffixes = [suffix for group in SAR.values() for suffix in group]\n",
    "    melted_log2FC[\"Suffix\"] = pd.Categorical(melted_log2FC[\"Suffix\"], categories=ordered_suffixes)\n",
    "    melted_log2FC[\"First.Protein.Description\"] = pd.Categorical(\n",
    "        melted_log2FC[\"First.Protein.Description\"],\n",
    "        categories=dendrogram_order,\n",
    "        ordered=True\n",
    "    )\n",
    "    melted_log2FC.sort_values(by=[\"Suffix\",\"First.Protein.Description\"], inplace=True)\n",
    "    \n",
    "    melted_log2FC.to_csv(\"temp.csv\")\n",
    "    \n",
    "    # Plotting\n",
    "    ax_bubble = fig.add_subplot(gs[1,0])\n",
    "    scatter = ax_bubble.scatter(\n",
    "        melted_log2FC[\"First.Protein.Description\"],  # x-axis as protein descriptions\n",
    "        melted_log2FC[\"Suffix\"],  # y-axis as suffixes\n",
    "        c=melted_log2FC[\"log2FC\"],  # Bubble color by log2FC\n",
    "        s=melted_log2FC[\"bh_FDR_log10\"] * 500,  # Bubble size proportional to -log10(FDR)\n",
    "        cmap=\"coolwarm\",  # Color map for log2FC\n",
    "        alpha=0.7,  # Transparency\n",
    "        edgecolors=\"w\",\n",
    "        clip_on=False\n",
    "    )\n",
    "    \n",
    "    # Define colors for each cluster\n",
    "    unique_clusters = melted_log2FC[\"Cluster\"].unique()\n",
    "    cluster_colors = {cluster: get_cmap(\"tab10\")(i / len(unique_clusters)) for i, cluster in enumerate(unique_clusters)}\n",
    "    melted_log2FC[\"Cluster_Color\"] = melted_log2FC[\"Cluster\"].map(cluster_colors)\n",
    "\n",
    "    # Color y-axis labels based on group mapping (keep y-axis coloring based on SAR)\n",
    "    x_labels = ax_bubble.get_xticklabels()\n",
    "    y_labels = ax_bubble.get_yticklabels()\n",
    "    for label in x_labels:\n",
    "        description = label.get_text()\n",
    "        if description in melted_log2FC[\"First.Protein.Description\"].values:\n",
    "            cluster_id = melted_log2FC.loc[melted_log2FC[\"First.Protein.Description\"] == description, \"Cluster\"].iloc[0]\n",
    "            label.set_color(cluster_colors[cluster_id])\n",
    "    for label in y_labels:\n",
    "        group = label.get_text()\n",
    "        if suffix_to_group.get(group) in group_colors:\n",
    "            label.set_color(group_colors[suffix_to_group[group]])\n",
    "    \n",
    "    # Colorbar and size legend (same as before)\n",
    "    ax_empty = fig.add_subplot(gs[0,1])\n",
    "    ax_empty.axis('off')\n",
    "    \n",
    "    gs_nested = GridSpecFromSubplotSpec(2, 1, subplot_spec=gs[1, 1], height_ratios=[2, 3])\n",
    "    ax_cbar = fig.add_subplot(gs_nested[1,:])\n",
    "    #ax_cbar.axis('off')\n",
    "    cbar = plt.colorbar(scatter,  cax=ax_cbar, shrink=0.8, location=\"left\", ticklocation=\"right\")\n",
    "    cbar.set_label(\"Log2FC Value\", fontsize=20, rotation=270, labelpad=30)\n",
    "    ax_cbar.tick_params(labelsize=18)  # Set tick label size (for horizontal colorbar)\n",
    "    ax_cbar.set_aspect(5)\n",
    "    \n",
    "    ax_legend = fig.add_subplot(gs_nested[0, 0])  # Top nested plot\n",
    "    ax_legend.axis(\"off\")\n",
    "    sizes = [1, 2, 3]  # log10(bh_FDR) values\n",
    "    size_labels = [f\"\\n\\nFDR = {10**-i}\\n\\n\" for i in sizes]\n",
    "    size_bubbles = [1, 2, 3]  # Corresponding bubble sizes\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='gray', markersize=2*np.sqrt(size*500/np.pi), label=label) for size, label in zip(size_bubbles, size_labels)]\n",
    "    ax_legend.legend(handles=handles, title=\"\", fontsize=16, labelspacing=0.05, borderaxespad=0.8, handleheight=0.2 # Space between axes and legend\n",
    "                     #handletextpad=3.0,  # Increase padding between handles and text\n",
    "                     #borderpad=2.0,  # Increase padding inside the legend box\n",
    "                       # Increase spacing between labels\n",
    "    )\n",
    "    \n",
    "    # Customize x and y labels, title, and layout\n",
    "    ax_bubble.set_xlabel(\"\")\n",
    "    ax_bubble.tick_params(axis='x', rotation=90, labelsize=20)\n",
    "    ax_bubble.set_xlim(-0.5, len(set(melted_log2FC[\"First.Protein.Description\"])) - 0.5)\n",
    "    \n",
    "    ax_bubble.set_ylabel(\"\")\n",
    "    ax_bubble.invert_yaxis()\n",
    "    ax_bubble.tick_params(axis='y', labelsize=20, pad=20)\n",
    "    \n",
    "    ax_dendro.set_title(\"VAV1 Glue Degradome\", fontsize=30, pad=10)\n",
    "    ax_bubble.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    \n",
    "    for spine in ax_bubble.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    \n",
    "    plt.subplots_adjust(hspace=0.01, wspace=0.01)  # Adjust the vertical space between the subplots\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"bubble_plot.png\",dpi=300)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble_dendro_plot(melted_log2FC=melted_log2FC, pivot_df=pivot_df, SAR=SAR, SAR_suffix=\"_1uM_vs_DMSO\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
